\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{MINST Kaggle Digit Recognizer: Contrasting the Randomized Forest andMLP Neural Network}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
   Andrew Osborne \\
   \texttt{amo004@uark.edu} \\
   \And
   Josh Price \\
   \texttt{jdp024@uark.edu} \\
   \AND
   April Walker \\
   \texttt{adw027@uark.edu} \\
   \\
   University of Arkansas, \\
   Fayetteville, AR, 72701, USA
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  In order to tackle the classic MINST dataset, our team developed a randomized forest and a multi-layer perception (MLP) classifier using the \verb+sklearn+ Python library. While our RF was able to achieve about 6\% test error with only 10 trees, increasing the number of trees soon developed diminishing returns. Cross validation never breached 97\% accuracy on test data, suggesting an RF is not well suited to solve such a high dimensional problem. In order to optimize our MLP NN, we explored the use of stochastic and Adam gradient descent, however found the later consistently outperformed the former. In order to balance accuracy vs. complexity, our cross validation method suggests the use of 1 hidden layer and 256 nodes. Our final Kaggle submission resulted in a correct prediction rate of 97.90\%, suggesting our NN is well-trained.    
\end{abstract}

\section{The MINST Dataset}

I'll probably talk about the considerations going into this project and the reason behind our chosen models. It might also be nice to just give a general background of the dataset that will set this up?

\section{Randomized Forest}

Here I will give a basic whatever on RF's that Dr. Zhang will already know. "It's like a decision tree with extra steps that gives you 'good enough' results." -Someone

\subsection{Implementation}

This is probably not the best title for this section

\subsection{Cross-Validation and Results}

\subsection{Future Considerations}
Because an RF is probably so so, but there are probably similar models that work better on problems w/ high levels of dimensionality. 

\section{Multi-Layer Perceptron Classifier}
This will be much more interesting to talk about

\subsection{Contrasting Gradient Descent Algorithms}
Because it will probably be good to talk about why Adam's so good, basically because it used the components of a lot of other good methods.

\subsection{Cross-Validation and Results}

\section{Conclusions}
MLP NN's are noice, Los Bobo's is better. 


\clearpage

\section*{References}

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references. {\bf Remember that you can use more than eight
  pages as long as the additional pages contain \emph{only} cited references.}
\medskip

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
